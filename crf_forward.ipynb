{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "from crf import *\n",
    "\n",
    "\n",
    "class CRFModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = config['dropout']\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.pad_value = config['pad_value']\n",
    "        self.CLS = config['CLS']\n",
    "        self.context_encoder = AutoModel.from_pretrained(\n",
    "            config['bert_path'])\n",
    "        self.dim = self.context_encoder.embeddings.word_embeddings.weight.data.shape[-1]\n",
    "        self.spk_embeddings = nn.Embedding(300, self.dim)\n",
    "        self.crf_layer = CRF(self.num_classes)\n",
    "        self.emission = nn.Linear(self.dim, self.num_classes)\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    def device(self):\n",
    "        return self.context_encoder.device\n",
    "\n",
    "    def forward(self, sentences, sentences_mask, speaker_ids, last_turns, emotion_idxs=None):\n",
    "        '''\n",
    "        sentences: batch * max_turns * max_length\n",
    "        speaker_ids: batch * max_turns\n",
    "        emotion[optional] : batch * max_turns\n",
    "        '''\n",
    "        batch_size = sentences.shape[0]\n",
    "        max_turns = sentences.shape[1]\n",
    "        max_len = sentences.shape[-1]\n",
    "        speaker_ids = speaker_ids.reshape(batch_size * max_turns, -1)\n",
    "        sentences = sentences.reshape(batch_size * max_turns, -1)\n",
    "        cls_id = torch.ones_like(speaker_ids) * self.CLS\n",
    "        input_ids = torch.cat((cls_id, sentences), 1)\n",
    "        mask = 1 - (input_ids == (self.pad_value)).long()\n",
    "        # with torch.no_grad():\n",
    "        utterance_encoded = self.context_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )['last_hidden_state']\n",
    "        mask_pos = mask.sum(1)-2\n",
    "        features = utterance_encoded[torch.arange(mask_pos.shape[0]), mask_pos, :]\n",
    "        emissions = self.emission(features)\n",
    "        crf_emissions = emissions.reshape(batch_size, max_turns, -1)\n",
    "        crf_emissions = crf_emissions.transpose(0, 1)\n",
    "        sentences_mask = sentences_mask.transpose(0, 1)\n",
    "        speaker_ids = speaker_ids.reshape(batch_size, max_turns).transpose(0, 1)\n",
    "        last_turns = last_turns.transpose(0, 1)\n",
    "        # train\n",
    "        if emotion_idxs is not None:\n",
    "            emotion_idxs = emotion_idxs.transpose(0, 1)\n",
    "            loss1 = -self.crf_layer(crf_emissions, emotion_idxs, mask=sentences_mask)\n",
    "            # 接上分类loss让CRF专注序列信息\n",
    "            loss2 = self.loss_func(emissions.view(-1, self.num_classes), emotion_idxs.view(-1))\n",
    "            loss = loss1 + loss2\n",
    "            return loss\n",
    "        # test\n",
    "        else:\n",
    "            return self.crf_layer.decode(crf_emissions, mask=sentences_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature -> \n",
    "mask_pos = mask.sum(1)-2 -> sum over horizontal row (diff columns) -> output [1,row]\n",
    "# change transpose(0,1) -> sentences_mask -> .reshape[batch_size, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤전 + 창현 버전\n",
    "class CRFModel(nn.Module):\n",
    "    def __init__(self, numClasses, dropout, bert_path):\n",
    "        super().__init__()\n",
    "        self.numClasses = numClasses\n",
    "        self.dropout = dropout\n",
    "        self.padValue = 1 # pad value\n",
    "        # CLS\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "        self.CLS = tokenizer('')['input_ids'][0]\n",
    "        self.encoder = AutoModel.from_pretrained(bert_path)\n",
    "        self.dimension = self.encoder.embeddings.word_embeddings.weight.data.shape[-1]\n",
    "        self.spkEmbeddings = nn.Embedding(300, self.dimension)\n",
    "        self.CRFlayer = CRF(self.numClasses)\n",
    "        self.emission = nn.Linear(self.dimension, self.numClasses)\n",
    "        self.lossFunc = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    def device(self):\n",
    "        return self.encoder.device\n",
    "\n",
    "    def forward(self, sentences, sentencesMask, speakerIds, lastTurns, emotionIdxes=None):\n",
    "        '''\n",
    "        sentences: batch * max_turns * max_length\n",
    "        speaker_ids: batch * max_turns\n",
    "        emotion[optional] : batch * max_turns\n",
    "        '''\n",
    "\n",
    "        #my implementation\n",
    "        sentBatchSize, sentMaxTurns, sentMaxLen = sentences.shape[0], sentences.shape[1], sentences.shape[2]\n",
    "        speakerBatchSize, speakerMaxTurns = speakerIds.shape[0], speakerIds.shape[1]\n",
    "\n",
    "        sentInputRowNum = sentBatchSize*sentMaxTurns\n",
    "        speakerInputRowNum = speakerBatchSize*speakerMaxTurns\n",
    "\n",
    "        sentencesReshaped = sentences.reshape(sentInputRowNum, -1)\n",
    "        speakerIdsReshaped = speakerIds.reshape(speakerInputRowNum, -1)\n",
    "\n",
    "\n",
    "        clsId = torch.ones(speakerIdsReshaped.size(), dtype=speakerIdsReshaped.dtype, \\\n",
    "                            layout=speakerIdsReshaped.layout, device=speakerIdsReshaped.device) * self.CLS\n",
    "        # clsId = torch.ones_like(speakerIdsReshaped) * self.CLS\n",
    "        inputIds = torch.concat(tensors=(clsId, sentencesReshaped), dim=1)\n",
    "        \n",
    "        # mask is used to avoid/ignore padded values of the input tensor\n",
    "        # masking indices should be {0: if padded, 1: if not padded}\n",
    "        inputIds[inputIds==self.padValue] = 0\n",
    "        inputIds[inputIds!=self.padValue] = 1\n",
    "        attentionMask = inputIds\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        utteranceEncoded = self.context_encoder(\n",
    "            input_ids=inputIds,\n",
    "            attention_mask=attentionMask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )['last_hidden_state']\n",
    "\n",
    "\n",
    "        maskPos = torch.sum(input=mask, dim=1, keepdim=False) - 2\n",
    "        # change below\n",
    "        features = utteranceEncoded[torch.arange(maskPos.shape[0]), maskPos, :]\n",
    "        emissions = self.emission(features)\n",
    "        crfEmissions = emissions.reshape(sentBatchSize, sentMaxTurns, -1).transpose(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        sentencesMask = torch.transpose(sentencesMask, dim0=0, dim1=1)\n",
    "        # check if it runs, if not it may mean speaker and sentence batch size are different\n",
    "        speakerIds = torch.transpose(speakerIds.reshape(speakerBatchSize, speakerMaxTurns), dim0=0, dim1=1) \n",
    "        lastTurns = torch.transpose(lastTurns, dim0=0, dim1=1)\n",
    "\n",
    "\n",
    "\n",
    "        # train\n",
    "        if emotionIdxes is not None:\n",
    "            emotionIdxes = emotionIdxes.transpose(0, 1)\n",
    "            return -self.CRFlayer(crfEmissions, emotionIdxes, mask=sentencesMask) + self.lossFunc(emissions.view(-1, self.numClasses), emotionIdxes.view(-1))\n",
    "        else:\n",
    "            return self.CRFlayer.decode(crfEmissions, mask=sentencesMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤전 버전\n",
    "class CRFModel(nn.Module):\n",
    "\n",
    "    def __init__(self, numClasses, dropout, bert_path):\n",
    "        super().__init__()\n",
    "        self.numClasses = numClasses\n",
    "        self.dropout = dropout\n",
    "        self.padValue = 1 # pad value\n",
    "        # CLS\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "        self.CLS = tokenizer('')['input_ids'][0]\n",
    "        self.encoder = AutoModel.from_pretrained(bert_path)\n",
    "        self.dimension = self.encoder.embeddings.word_embeddings.weight.data.shape[-1]\n",
    "        self.spkEmbeddings = nn.Embedding(300, self.dimension)\n",
    "        self.CRFlayer = CRF(self.numClasses)\n",
    "        self.emission = nn.Linear(self.dimension, self.numClasses)\n",
    "        self.lossFunc = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    def device(self):\n",
    "        return self.encoder.device\n",
    "\n",
    "    def forward(self, sentences, sentencesMask, speakerIds, lastTurns, emotionIdxes=None):\n",
    "    #def forward(self, sentences, sentencesMask, speakerIds, emotionIdxes):\n",
    "        batchSize, maxTurns = sentences.shape[0], sentences.shape[1]\n",
    "\n",
    "        speakerIdsReshaped = speakerIds.reshape(batchSize * maxTurns, -1)\n",
    "        sentencesReshaped = sentences.reshape(batchSize * maxTurns, -1)\n",
    "\n",
    "        clsId = torch.ones_like(speakerIdsReshaped) * self.CLS\n",
    "        inputIds = torch.cat((clsId, sentencesReshaped), 1)\n",
    "        mask = 1 - (inputIds == (self.padValue)).long()\n",
    "\n",
    "        utteranceEncoded = self.encoder(\n",
    "            input_ids=inputIds,\n",
    "            attention_mask=mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )['last_hidden_state']\n",
    "\n",
    "        maskPos = mask.sum(1)-2\n",
    "        features = utteranceEncoded[torch.arange(maskPos.shape[0]), maskPos, :]\n",
    "        emissions = self.emission(features)\n",
    "        crfEmissions = emissions.reshape(batchSize, maxTurns, -1).transpose(0, 1)\n",
    "        sentencesMask = sentencesMask.transpose(0, 1)\n",
    "        speakerIds = speakerIds.reshape(batchSize, maxTurns).transpose(0, 1)\n",
    "        lastTurns = lastTurns.transpose(0, 1)\n",
    "\n",
    "        # train\n",
    "        if emotionIdxes is not None:\n",
    "            emotionIdxes = emotionIdxes.transpose(0, 1)\n",
    "            return -self.CRFlayer(crfEmissions, emotionIdxes, mask=sentencesMask) + self.lossFunc(emissions.view(-1, self.numClasses), emotionIdxes.view(-1))\n",
    "        else:\n",
    "            return self.CRFlayer.decode(crfEmissions, mask=sentencesMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2., -1.,  0.],\n",
      "        [ 1., -1.,  0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[2., -1., 0], [1., -1., 0]])\n",
    "print(a)\n",
    "a == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0],\n",
       "        [1, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bool().int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [False, False,  True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.where(a == 2, 0, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pos=a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 1.],\n",
       "        [0., 2., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(True).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a96337b70dab0eb2bcaed0ba38f8b8a9f8193ff32d1d954b22771c4e1a3afc5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
